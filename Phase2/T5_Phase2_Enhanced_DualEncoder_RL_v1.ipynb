{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Enhanced CodeT5 with Dual-Encoder Architecture + Reinforcement Learning\n",
    "\n",
    "**Research Project**: Enhanced Kubernetes Configuration Generation through Dual-Encoder CodeT5\n",
    "\n",
    "**Phase**: 2 - Enhanced Architecture Implementation (Weeks 5-6)\n",
    "\n",
    "**Objectives**:\n",
    "1. Implement dual-encoder architecture (Intent + K8s Pattern encoders)\n",
    "2. Add RL optimization using BP% as primary reward signal\n",
    "3. Comparative evaluation: Baseline vs Enhanced model\n",
    "4. Achieve targets: CodeBLEU ≥85%, BP% ≥90%, Quality ≥85/100\n",
    "\n",
    "**Building on**:\n",
    "- Baseline: CodeBLEU 83.43%, YAML Validity 90.62%, BP% ~55%\n",
    "- Phase 1: Best Practices Analyzer (11 checks), Feedback System\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.30.0 torch==2.0.1 datasets==2.12.0 \n",
    "!pip install pyyaml==6.0 rouge-score==0.1.2 nltk==3.8.1\n",
    "!pip install sacrebleu==2.3.1 evaluate==0.4.0\n",
    "!pip install codebleu==0.1.1  # For CodeBLEU evaluation\n",
    "!pip install tensorboard==2.13.0  # For training visualization\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/ravali/thesis-research'\n",
    "BASELINE_MODEL_PATH = f'{PROJECT_ROOT}/results/baseline_model'\n",
    "ENHANCED_MODEL_PATH = f'{PROJECT_ROOT}/results/enhanced_model'\n",
    "DATA_PATH = f'{PROJECT_ROOT}/data'\n",
    "RESULTS_PATH = f'{PROJECT_ROOT}/results/phase2'\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p {ENHANCED_MODEL_PATH}\n",
    "!mkdir -p {RESULTS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    T5Config,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Phase 1 Components\n",
    "\n",
    "We'll import the Best Practices Analyzer from Phase 1 to use as part of the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices Analyzer from Phase 1\n",
    "class KubernetesBestPracticesAnalyzer:\n",
    "    \"\"\"Analyzes Kubernetes YAML against 11 critical best practices.\n",
    "    \n",
    "    This is imported from Phase 1 (T5_Phase1_Feedback_System_v1.ipynb)\n",
    "    and used in the RL reward function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.best_practices = [\n",
    "            'namespace',\n",
    "            'labels', \n",
    "            'resources',\n",
    "            'probes',\n",
    "            'security_context',\n",
    "            'readonly_fs',\n",
    "            'capabilities',\n",
    "            'image_tag',\n",
    "            'service_account',\n",
    "            'network_policy',\n",
    "            'pod_security'\n",
    "        ]\n",
    "    \n",
    "    def check_namespace(self, yaml_dict):\n",
    "        \"\"\"Check if namespace is defined.\"\"\"\n",
    "        return yaml_dict.get('metadata', {}).get('namespace') is not None\n",
    "    \n",
    "    def check_labels(self, yaml_dict):\n",
    "        \"\"\"Check for proper labels (app, version, component).\"\"\"\n",
    "        labels = yaml_dict.get('metadata', {}).get('labels', {})\n",
    "        return 'app' in labels or 'version' in labels or 'component' in labels\n",
    "    \n",
    "    def check_resources(self, yaml_dict):\n",
    "        \"\"\"Check for resource limits and requests.\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True  # Only check Deployments\n",
    "        \n",
    "        containers = yaml_dict.get('spec', {}).get('template', {}).get('spec', {}).get('containers', [])\n",
    "        for container in containers:\n",
    "            resources = container.get('resources', {})\n",
    "            if not resources.get('limits') or not resources.get('requests'):\n",
    "                return False\n",
    "        return len(containers) > 0\n",
    "    \n",
    "    def check_probes(self, yaml_dict):\n",
    "        \"\"\"Check for liveness and readiness probes.\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True\n",
    "        \n",
    "        containers = yaml_dict.get('spec', {}).get('template', {}).get('spec', {}).get('containers', [])\n",
    "        for container in containers:\n",
    "            if not container.get('livenessProbe') or not container.get('readinessProbe'):\n",
    "                return False\n",
    "        return len(containers) > 0\n",
    "    \n",
    "    def check_security_context(self, yaml_dict):\n",
    "        \"\"\"Check for security context (runAsNonRoot).\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True\n",
    "        \n",
    "        security_context = yaml_dict.get('spec', {}).get('template', {}).get('spec', {}).get('securityContext', {})\n",
    "        return security_context.get('runAsNonRoot') == True\n",
    "    \n",
    "    def check_readonly_fs(self, yaml_dict):\n",
    "        \"\"\"Check for read-only root filesystem.\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True\n",
    "        \n",
    "        containers = yaml_dict.get('spec', {}).get('template', {}).get('spec', {}).get('containers', [])\n",
    "        for container in containers:\n",
    "            sec_ctx = container.get('securityContext', {})\n",
    "            if not sec_ctx.get('readOnlyRootFilesystem'):\n",
    "                return False\n",
    "        return len(containers) > 0\n",
    "    \n",
    "    def check_capabilities(self, yaml_dict):\n",
    "        \"\"\"Check if dangerous capabilities are dropped.\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True\n",
    "        \n",
    "        containers = yaml_dict.get('spec', {}).get('template', {}).get('spec', {}).get('containers', [])\n",
    "        for container in containers:\n",
    "            sec_ctx = container.get('securityContext', {})\n",
    "            caps = sec_ctx.get('capabilities', {})\n",
    "            if not caps.get('drop'):\n",
    "                return False\n",
    "        return len(containers) > 0\n",
    "    \n",
    "    def check_image_tag(self, yaml_dict):\n",
    "        \"\"\"Check that image doesn't use :latest tag.\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True\n",
    "        \n",
    "        containers = yaml_dict.get('spec', {}).get('template', {}).get('spec', {}).get('containers', [])\n",
    "        for container in containers:\n",
    "            image = container.get('image', '')\n",
    "            if ':latest' in image or ':' not in image:\n",
    "                return False\n",
    "        return len(containers) > 0\n",
    "    \n",
    "    def check_service_account(self, yaml_dict):\n",
    "        \"\"\"Check if custom service account is defined.\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True\n",
    "        \n",
    "        service_account = yaml_dict.get('spec', {}).get('template', {}).get('spec', {}).get('serviceAccountName')\n",
    "        return service_account is not None and service_account != 'default'\n",
    "    \n",
    "    def check_network_policy(self, yaml_dict):\n",
    "        \"\"\"For NetworkPolicy resources, check basic structure.\"\"\"\n",
    "        if yaml_dict.get('kind') == 'NetworkPolicy':\n",
    "            return 'spec' in yaml_dict and 'podSelector' in yaml_dict.get('spec', {})\n",
    "        return True  # Pass for non-NetworkPolicy resources\n",
    "    \n",
    "    def check_pod_security(self, yaml_dict):\n",
    "        \"\"\"Check for pod security standards compliance.\"\"\"\n",
    "        if yaml_dict.get('kind') != 'Deployment':\n",
    "            return True\n",
    "        \n",
    "        pod_spec = yaml_dict.get('spec', {}).get('template', {}).get('spec', {})\n",
    "        # Check for hostNetwork, hostPID, hostIPC - should all be false or absent\n",
    "        dangerous = pod_spec.get('hostNetwork') or pod_spec.get('hostPID') or pod_spec.get('hostIPC')\n",
    "        return not dangerous\n",
    "    \n",
    "    def analyze(self, yaml_content):\n",
    "        \"\"\"Analyze YAML against all best practices.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'checks': {practice: bool},\n",
    "                'passed': int,\n",
    "                'total': int,\n",
    "                'percentage': float,\n",
    "                'score': float  # 0-100\n",
    "            }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(yaml_content, str):\n",
    "                yaml_dict = yaml.safe_load(yaml_content)\n",
    "            else:\n",
    "                yaml_dict = yaml_content\n",
    "            \n",
    "            if not yaml_dict or not isinstance(yaml_dict, dict):\n",
    "                return self._empty_result()\n",
    "            \n",
    "            checks = {}\n",
    "            checks['namespace'] = self.check_namespace(yaml_dict)\n",
    "            checks['labels'] = self.check_labels(yaml_dict)\n",
    "            checks['resources'] = self.check_resources(yaml_dict)\n",
    "            checks['probes'] = self.check_probes(yaml_dict)\n",
    "            checks['security_context'] = self.check_security_context(yaml_dict)\n",
    "            checks['readonly_fs'] = self.check_readonly_fs(yaml_dict)\n",
    "            checks['capabilities'] = self.check_capabilities(yaml_dict)\n",
    "            checks['image_tag'] = self.check_image_tag(yaml_dict)\n",
    "            checks['service_account'] = self.check_service_account(yaml_dict)\n",
    "            checks['network_policy'] = self.check_network_policy(yaml_dict)\n",
    "            checks['pod_security'] = self.check_pod_security(yaml_dict)\n",
    "            \n",
    "            passed = sum(checks.values())\n",
    "            total = len(checks)\n",
    "            percentage = (passed / total) * 100\n",
    "            \n",
    "            return {\n",
    "                'checks': checks,\n",
    "                'passed': passed,\n",
    "                'total': total,\n",
    "                'percentage': percentage,\n",
    "                'score': percentage  # Normalized to 0-100\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing YAML: {e}\")\n",
    "            return self._empty_result()\n",
    "    \n",
    "    def _empty_result(self):\n",
    "        return {\n",
    "            'checks': {bp: False for bp in self.best_practices},\n",
    "            'passed': 0,\n",
    "            'total': len(self.best_practices),\n",
    "            'percentage': 0.0,\n",
    "            'score': 0.0\n",
    "        }\n",
    "\n",
    "# Initialize analyzer\n",
    "bp_analyzer = KubernetesBestPracticesAnalyzer()\n",
    "print(\"✓ Best Practices Analyzer loaded from Phase 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dual-Encoder Architecture\n",
    "\n",
    "### Architecture Design:\n",
    "\n",
    "```\n",
    "Natural Language Intent → [Intent Encoder] ─┐\n",
    "                                             ├→ [Unified Decoder] → Kubernetes YAML\n",
    "K8s Pattern Context    → [K8s Encoder]    ─┘\n",
    "```\n",
    "\n",
    "**Key Innovation**: Two specialized encoders that:\n",
    "1. **Intent Encoder**: Processes natural language deployment requirements\n",
    "2. **K8s Pattern Encoder**: Processes Kubernetes configuration patterns and syntax\n",
    "3. **Attention Fusion**: Combines both representations for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderCodeT5(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced CodeT5 with Dual-Encoder Architecture.\n",
    "    \n",
    "    Architecture:\n",
    "    - Intent Encoder: CodeT5 encoder for natural language intent\n",
    "    - K8s Pattern Encoder: Separate CodeT5 encoder for Kubernetes patterns\n",
    "    - Attention Fusion Layer: Combines both encoder outputs\n",
    "    - Unified Decoder: Standard CodeT5 decoder\n",
    "    \n",
    "    This architecture improves intent-configuration alignment compared to\n",
    "    single-encoder baseline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_name='Salesforce/codet5-base'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load base CodeT5 model and config\n",
    "        self.config = T5Config.from_pretrained(base_model_name)\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        # Intent Encoder - processes natural language requirements\n",
    "        self.intent_encoder = base_model.encoder\n",
    "        \n",
    "        # K8s Pattern Encoder - processes Kubernetes configuration patterns\n",
    "        # Initialize as a copy of the intent encoder\n",
    "        from copy import deepcopy\n",
    "        self.k8s_encoder = deepcopy(base_model.encoder)\n",
    "        \n",
    "        # Decoder - unified decoder that receives fused encoder outputs\n",
    "        self.decoder = base_model.decoder\n",
    "        self.lm_head = base_model.lm_head\n",
    "        \n",
    "        # Attention Fusion Layer - combines both encoder outputs\n",
    "        hidden_size = self.config.d_model\n",
    "        self.fusion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layers for combining encoders\n",
    "        self.intent_projection = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k8s_projection = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fusion_projection = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        \n",
    "        # Layer norm for stability\n",
    "        self.fusion_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        print(\"✓ Dual-Encoder Architecture initialized\")\n",
    "        print(f\"  - Intent Encoder: {sum(p.numel() for p in self.intent_encoder.parameters())/1e6:.1f}M params\")\n",
    "        print(f\"  - K8s Encoder: {sum(p.numel() for p in self.k8s_encoder.parameters())/1e6:.1f}M params\")\n",
    "        print(f\"  - Decoder: {sum(p.numel() for p in self.decoder.parameters())/1e6:.1f}M params\")\n",
    "        print(f\"  - Total: {sum(p.numel() for p in self.parameters())/1e6:.1f}M params\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        intent_input_ids,\n",
    "        intent_attention_mask,\n",
    "        k8s_input_ids,\n",
    "        k8s_attention_mask,\n",
    "        labels=None,\n",
    "        decoder_attention_mask=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass through dual-encoder architecture.\n",
    "        \n",
    "        Args:\n",
    "            intent_input_ids: Tokenized natural language intent\n",
    "            intent_attention_mask: Attention mask for intent\n",
    "            k8s_input_ids: Tokenized K8s pattern context\n",
    "            k8s_attention_mask: Attention mask for K8s patterns\n",
    "            labels: Target YAML tokens (for training)\n",
    "            decoder_attention_mask: Decoder attention mask\n",
    "        \n",
    "        Returns:\n",
    "            dict with 'loss' and 'logits'\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode intent\n",
    "        intent_outputs = self.intent_encoder(\n",
    "            input_ids=intent_input_ids,\n",
    "            attention_mask=intent_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        intent_hidden = intent_outputs.last_hidden_state\n",
    "        \n",
    "        # Encode K8s patterns\n",
    "        k8s_outputs = self.k8s_encoder(\n",
    "            input_ids=k8s_input_ids,\n",
    "            attention_mask=k8s_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        k8s_hidden = k8s_outputs.last_hidden_state\n",
    "        \n",
    "        # Fuse encoder outputs\n",
    "        fused_hidden = self._fuse_encoders(\n",
    "            intent_hidden, \n",
    "            k8s_hidden,\n",
    "            intent_attention_mask,\n",
    "            k8s_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Create combined attention mask\n",
    "        batch_size = intent_input_ids.shape[0]\n",
    "        combined_mask = torch.cat([\n",
    "            intent_attention_mask,\n",
    "            k8s_attention_mask\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=labels[:, :-1] if labels is not None else None,\n",
    "            encoder_hidden_states=fused_hidden,\n",
    "            encoder_attention_mask=combined_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Generate logits\n",
    "        logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        \n",
    "        # Compute loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels[:, 1:].reshape(-1)\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }\n",
    "    \n",
    "    def _fuse_encoders(self, intent_hidden, k8s_hidden, intent_mask, k8s_mask):\n",
    "        \"\"\"\n",
    "        Fuse intent and K8s encoder outputs using attention mechanism.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Project both encoder outputs\n",
    "        2. Apply cross-attention between them\n",
    "        3. Concatenate and project to final representation\n",
    "        \"\"\"\n",
    "        # Project encoder outputs\n",
    "        intent_proj = self.intent_projection(intent_hidden)\n",
    "        k8s_proj = self.k8s_projection(k8s_hidden)\n",
    "        \n",
    "        # Apply cross-attention: intent attends to K8s patterns\n",
    "        attended, _ = self.fusion_attention(\n",
    "            query=intent_proj,\n",
    "            key=k8s_proj,\n",
    "            value=k8s_proj,\n",
    "            key_padding_mask=(k8s_mask == 0)\n",
    "        )\n",
    "        \n",
    "        # Concatenate intent and attended K8s representations\n",
    "        combined = torch.cat([intent_proj, attended], dim=-1)\n",
    "        \n",
    "        # Project to final dimension\n",
    "        fused = self.fusion_projection(combined)\n",
    "        fused = self.fusion_norm(fused)\n",
    "        \n",
    "        # Concatenate both encoder outputs for decoder\n",
    "        # This gives decoder access to both intent and K8s patterns\n",
    "        final_hidden = torch.cat([fused, k8s_hidden], dim=1)\n",
    "        \n",
    "        return final_hidden\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        intent_input_ids,\n",
    "        intent_attention_mask,\n",
    "        k8s_input_ids,\n",
    "        k8s_attention_mask,\n",
    "        max_length=512,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate Kubernetes YAML from intent and K8s pattern inputs.\n",
    "        \"\"\"\n",
    "        # Encode both inputs\n",
    "        intent_outputs = self.intent_encoder(\n",
    "            input_ids=intent_input_ids,\n",
    "            attention_mask=intent_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        intent_hidden = intent_outputs.last_hidden_state\n",
    "        \n",
    "        k8s_outputs = self.k8s_encoder(\n",
    "            input_ids=k8s_input_ids,\n",
    "            attention_mask=k8s_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        k8s_hidden = k8s_outputs.last_hidden_state\n",
    "        \n",
    "        # Fuse encoders\n",
    "        fused_hidden = self._fuse_encoders(\n",
    "            intent_hidden,\n",
    "            k8s_hidden,\n",
    "            intent_attention_mask,\n",
    "            k8s_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Combined attention mask\n",
    "        combined_mask = torch.cat([\n",
    "            intent_attention_mask,\n",
    "            k8s_attention_mask\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Generate using decoder\n",
    "        # Note: This is a simplified version. Full implementation would need\n",
    "        # custom generation logic or adaptation of HuggingFace generation utilities\n",
    "        generated_ids = self.decoder.generate(\n",
    "            encoder_hidden_states=fused_hidden,\n",
    "            encoder_attention_mask=combined_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=early_stopping\n",
    "        )\n",
    "        \n",
    "        return generated_ids\n",
    "\n",
    "print(\"✓ Dual-Encoder CodeT5 architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation for Dual-Encoder Training\n",
    "\n",
    "We need to prepare two inputs for each training example:\n",
    "1. **Intent Input**: Natural language description\n",
    "2. **K8s Pattern Input**: Kubernetes pattern template or context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for dual-encoder training.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - intent: Natural language deployment requirement\n",
    "    - k8s_pattern: Kubernetes configuration pattern/template\n",
    "    - target_yaml: Ground truth Kubernetes YAML\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load training data\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} training examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Intent input (natural language)\n",
    "        intent = item['intent']\n",
    "        intent_encoding = self.tokenizer(\n",
    "            intent,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # K8s pattern input\n",
    "        # Extract kind and basic structure as pattern\n",
    "        k8s_pattern = self._extract_k8s_pattern(item['yaml'])\n",
    "        k8s_encoding = self.tokenizer(\n",
    "            k8s_pattern,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Target YAML\n",
    "        target_encoding = self.tokenizer(\n",
    "            item['yaml'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'intent_input_ids': intent_encoding['input_ids'].squeeze(),\n",
    "            'intent_attention_mask': intent_encoding['attention_mask'].squeeze(),\n",
    "            'k8s_input_ids': k8s_encoding['input_ids'].squeeze(),\n",
    "            'k8s_attention_mask': k8s_encoding['attention_mask'].squeeze(),\n",
    "            'labels': target_encoding['input_ids'].squeeze(),\n",
    "            'target_yaml': item['yaml']  # Keep for evaluation\n",
    "        }\n",
    "    \n",
    "    def _extract_k8s_pattern(self, yaml_str):\n",
    "        \"\"\"\n",
    "        Extract Kubernetes pattern template from full YAML.\n",
    "        \n",
    "        Strategy: Create a simplified template that captures:\n",
    "        - Resource kind\n",
    "        - Basic structure\n",
    "        - Common field names\n",
    "        \n",
    "        This helps the K8s encoder learn configuration patterns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            yaml_dict = yaml.safe_load(yaml_str)\n",
    "            \n",
    "            kind = yaml_dict.get('kind', 'Unknown')\n",
    "            api_version = yaml_dict.get('apiVersion', 'v1')\n",
    "            \n",
    "            # Build pattern based on kind\n",
    "            if kind == 'Deployment':\n",
    "                pattern = f\"\"\"\n",
    "apiVersion: {api_version}\n",
    "kind: {kind}\n",
    "metadata:\n",
    "  name: <NAME>\n",
    "  namespace: <NAMESPACE>\n",
    "  labels: <LABELS>\n",
    "spec:\n",
    "  replicas: <REPLICAS>\n",
    "  selector:\n",
    "    matchLabels: <LABELS>\n",
    "  template:\n",
    "    metadata:\n",
    "      labels: <LABELS>\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: <CONTAINER_NAME>\n",
    "        image: <IMAGE>\n",
    "        ports: <PORTS>\n",
    "        resources: <RESOURCES>\n",
    "        livenessProbe: <PROBE>\n",
    "        readinessProbe: <PROBE>\n",
    "        securityContext: <SECURITY>\n",
    "\"\"\"\n",
    "            elif kind == 'Service':\n",
    "                pattern = f\"\"\"\n",
    "apiVersion: {api_version}\n",
    "kind: {kind}\n",
    "metadata:\n",
    "  name: <NAME>\n",
    "  namespace: <NAMESPACE>\n",
    "spec:\n",
    "  type: <SERVICE_TYPE>\n",
    "  selector: <SELECTOR>\n",
    "  ports: <PORTS>\n",
    "\"\"\"\n",
    "            else:\n",
    "                pattern = f\"apiVersion: {api_version}\\nkind: {kind}\\nmetadata: <METADATA>\\nspec: <SPEC>\"\n",
    "            \n",
    "            return pattern.strip()\n",
    "            \n",
    "        except:\n",
    "            return \"kind: Unknown\"\n",
    "\n",
    "print(\"✓ Dual-Encoder Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reinforcement Learning Optimization\n",
    "\n",
    "### Reward Function (from Professor Meeting Nov 06):\n",
    "\n",
    "```python\n",
    "reward = 0.3 * CodeBLEU + 0.4 * BP% + 0.2 * Security + 0.1 * Complexity\n",
    "```\n",
    "\n",
    "**Key Insight**: BP% weighted highest (40%) because it captures security, reliability, and operational readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCalculator:\n",
    "    \"\"\"\n",
    "    Computes multi-dimensional reward for RL optimization.\n",
    "    \n",
    "    Reward Function:\n",
    "        R = 0.3 * CodeBLEU + 0.4 * BP% + 0.2 * Security + 0.1 * Complexity\n",
    "    \n",
    "    Weights based on professor feedback (Nov 06 meeting):\n",
    "    - BP%: 40% (highest) - captures production readiness\n",
    "    - CodeBLEU: 30% - measures generation quality\n",
    "    - Security: 20% - critical for production\n",
    "    - Complexity: 10% - lower weight, quality over simplicity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bp_analyzer = KubernetesBestPracticesAnalyzer()\n",
    "        \n",
    "        # Import codebleu for evaluation\n",
    "        try:\n",
    "            from codebleu import calc_codebleu\n",
    "            self.calc_codebleu = calc_codebleu\n",
    "        except:\n",
    "            print(\"Warning: codebleu not available, using approximate BLEU\")\n",
    "            self.calc_codebleu = None\n",
    "    \n",
    "    def compute_reward(self, generated_yaml, ground_truth_yaml):\n",
    "        \"\"\"\n",
    "        Compute comprehensive reward signal.\n",
    "        \n",
    "        Args:\n",
    "            generated_yaml: YAML string generated by model\n",
    "            ground_truth_yaml: Reference YAML string\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'total_reward': float (0-1),\n",
    "                'codebleu_score': float,\n",
    "                'bp_score': float,\n",
    "                'security_score': float,\n",
    "                'complexity_score': float\n",
    "            }\n",
    "        \"\"\"\n",
    "        # 1. CodeBLEU Score (30%)\n",
    "        codebleu_score = self._compute_codebleu(generated_yaml, ground_truth_yaml)\n",
    "        \n",
    "        # 2. Best Practices Score (40%) - KEY METRIC\n",
    "        bp_result = self.bp_analyzer.analyze(generated_yaml)\n",
    "        bp_score = bp_result['percentage'] / 100.0  # Normalize to 0-1\n",
    "        \n",
    "        # 3. Security Score (20%)\n",
    "        security_score = self._compute_security_score(generated_yaml, bp_result)\n",
    "        \n",
    "        # 4. Complexity Score (10%)\n",
    "        complexity_score = self._compute_complexity_score(generated_yaml)\n",
    "        \n",
    "        # Compute weighted total reward\n",
    "        total_reward = (\n",
    "            0.3 * codebleu_score +\n",
    "            0.4 * bp_score +\n",
    "            0.2 * security_score +\n",
    "            0.1 * complexity_score\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'total_reward': total_reward,\n",
    "            'codebleu_score': codebleu_score,\n",
    "            'bp_score': bp_score,\n",
    "            'bp_percentage': bp_result['percentage'],\n",
    "            'security_score': security_score,\n",
    "            'complexity_score': complexity_score\n",
    "        }\n",
    "    \n",
    "    def _compute_codebleu(self, generated, reference):\n",
    "        \"\"\"Compute CodeBLEU score.\"\"\"\n",
    "        try:\n",
    "            if self.calc_codebleu:\n",
    "                result = self.calc_codebleu(\n",
    "                    [reference],\n",
    "                    [generated],\n",
    "                    lang='yaml'\n",
    "                )\n",
    "                return result['codebleu']\n",
    "            else:\n",
    "                # Fallback: simple token overlap\n",
    "                gen_tokens = set(generated.split())\n",
    "                ref_tokens = set(reference.split())\n",
    "                if not ref_tokens:\n",
    "                    return 0.0\n",
    "                overlap = len(gen_tokens & ref_tokens) / len(ref_tokens)\n",
    "                return min(overlap, 1.0)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _compute_security_score(self, yaml_str, bp_result):\n",
    "        \"\"\"\n",
    "        Compute security score based on security-related best practices.\n",
    "        \n",
    "        Security checks:\n",
    "        - Security context (runAsNonRoot)\n",
    "        - Read-only filesystem\n",
    "        - Dropped capabilities\n",
    "        - No :latest tags\n",
    "        - Pod security (no hostNetwork, etc.)\n",
    "        \"\"\"\n",
    "        security_checks = [\n",
    "            'security_context',\n",
    "            'readonly_fs',\n",
    "            'capabilities',\n",
    "            'image_tag',\n",
    "            'pod_security'\n",
    "        ]\n",
    "        \n",
    "        passed = sum(\n",
    "            bp_result['checks'].get(check, False) \n",
    "            for check in security_checks\n",
    "        )\n",
    "        \n",
    "        return passed / len(security_checks)\n",
    "    \n",
    "    def _compute_complexity_score(self, yaml_str):\n",
    "        \"\"\"\n",
    "        Compute inverse complexity score.\n",
    "        Lower complexity = higher score.\n",
    "        \n",
    "        Measures:\n",
    "        - Number of lines\n",
    "        - Nesting depth\n",
    "        - Number of fields\n",
    "        \"\"\"\n",
    "        try:\n",
    "            yaml_dict = yaml.safe_load(yaml_str)\n",
    "            if not yaml_dict:\n",
    "                return 0.0\n",
    "            \n",
    "            # Count lines\n",
    "            lines = len(yaml_str.strip().split('\\n'))\n",
    "            \n",
    "            # Estimate nesting depth\n",
    "            max_depth = self._get_max_depth(yaml_dict)\n",
    "            \n",
    "            # Normalize scores (inverse for complexity)\n",
    "            # Target: 20-50 lines, depth 3-5\n",
    "            line_score = 1.0 if 20 <= lines <= 50 else max(0, 1.0 - abs(lines - 35) / 50)\n",
    "            depth_score = 1.0 if 3 <= max_depth <= 5 else max(0, 1.0 - abs(max_depth - 4) / 5)\n",
    "            \n",
    "            return (line_score + depth_score) / 2\n",
    "            \n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _get_max_depth(self, obj, current_depth=0):\n",
    "        \"\"\"Recursively compute maximum nesting depth.\"\"\"\n",
    "        if not isinstance(obj, dict):\n",
    "            return current_depth\n",
    "        \n",
    "        if not obj:\n",
    "            return current_depth + 1\n",
    "        \n",
    "        return max(\n",
    "            self._get_max_depth(v, current_depth + 1) \n",
    "            for v in obj.values()\n",
    "        )\n",
    "\n",
    "# Initialize reward calculator\n",
    "reward_calculator = RewardCalculator()\n",
    "print(\"✓ RL Reward Calculator initialized\")\n",
    "print(\"  Reward weights: CodeBLEU=30%, BP=40%, Security=20%, Complexity=10%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientTrainer:\n",
    "    \"\"\"\n",
    "    RL Trainer using Policy Gradient (REINFORCE algorithm).\n",
    "    \n",
    "    Training loop:\n",
    "    1. Generate YAML from model\n",
    "    2. Compute reward signal\n",
    "    3. Update model to maximize expected reward\n",
    "    4. Repeat\n",
    "    \n",
    "    This optimizes the model beyond supervised learning to maximize\n",
    "    domain-specific quality metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        reward_calculator,\n",
    "        learning_rate=1e-5,\n",
    "        gamma=0.99  # Discount factor\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reward_calculator = reward_calculator\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Optimizer for RL fine-tuning\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_bp_scores = []\n",
    "    \n",
    "    def train_episode(self, intent_batch, k8s_batch, target_batch):\n",
    "        \"\"\"\n",
    "        Train one RL episode (batch).\n",
    "        \n",
    "        Args:\n",
    "            intent_batch: Intent input IDs and masks\n",
    "            k8s_batch: K8s pattern input IDs and masks\n",
    "            target_batch: Ground truth YAML strings\n",
    "        \n",
    "        Returns:\n",
    "            dict with episode metrics\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Generate YAML from current policy\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                intent_input_ids=intent_batch['input_ids'],\n",
    "                intent_attention_mask=intent_batch['attention_mask'],\n",
    "                k8s_input_ids=k8s_batch['input_ids'],\n",
    "                k8s_attention_mask=k8s_batch['attention_mask'],\n",
    "                max_length=512,\n",
    "                num_beams=2,  # Faster for RL training\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode generated YAML\n",
    "        generated_yamls = [\n",
    "            self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in generated_ids\n",
    "        ]\n",
    "        \n",
    "        # Compute rewards for each generated YAML\n",
    "        rewards = []\n",
    "        bp_scores = []\n",
    "        \n",
    "        for gen_yaml, target_yaml in zip(generated_yamls, target_batch):\n",
    "            reward_dict = self.reward_calculator.compute_reward(gen_yaml, target_yaml)\n",
    "            rewards.append(reward_dict['total_reward'])\n",
    "            bp_scores.append(reward_dict['bp_percentage'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        rewards_tensor = torch.tensor(rewards, device=device)\n",
    "        \n",
    "        # Compute policy gradient loss\n",
    "        # Forward pass to get log probabilities\n",
    "        outputs = self.model(\n",
    "            intent_input_ids=intent_batch['input_ids'],\n",
    "            intent_attention_mask=intent_batch['attention_mask'],\n",
    "            k8s_input_ids=k8s_batch['input_ids'],\n",
    "            k8s_attention_mask=k8s_batch['attention_mask'],\n",
    "            labels=generated_ids  # Use generated as pseudo-labels\n",
    "        )\n",
    "        \n",
    "        # Policy gradient: maximize log_prob * reward\n",
    "        # Loss = -mean(log_prob * reward)\n",
    "        log_probs = F.log_softmax(outputs['logits'], dim=-1)\n",
    "        \n",
    "        # Get log probs of generated tokens\n",
    "        generated_log_probs = torch.gather(\n",
    "            log_probs,\n",
    "            dim=-1,\n",
    "            index=generated_ids.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "        \n",
    "        # Policy gradient loss with reward weighting\n",
    "        pg_loss = -(generated_log_probs * rewards_tensor.unsqueeze(-1)).mean()\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        pg_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        avg_reward = np.mean(rewards)\n",
    "        avg_bp = np.mean(bp_scores)\n",
    "        \n",
    "        self.episode_rewards.append(avg_reward)\n",
    "        self.episode_bp_scores.append(avg_bp)\n",
    "        \n",
    "        return {\n",
    "            'pg_loss': pg_loss.item(),\n",
    "            'avg_reward': avg_reward,\n",
    "            'avg_bp_score': avg_bp,\n",
    "            'min_reward': min(rewards),\n",
    "            'max_reward': max(rewards)\n",
    "        }\n",
    "\n",
    "print(\"✓ Policy Gradient Trainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Pipeline\n",
    "\n",
    "### Two-Stage Training:\n",
    "1. **Stage 1**: Supervised fine-tuning (warm-start)\n",
    "2. **Stage 2**: RL optimization (maximize rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "print(\"Loading CodeT5-base...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = DualEncoderCodeT5('Salesforce/codet5-base')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n✓ Model loaded with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "print(\"Loading training data...\")\n",
    "train_dataset = DualEncoderDataset(\n",
    "    data_path=f'{DATA_PATH}/train_data.json',\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"✓ Data loaded: {len(train_dataset)} examples, {len(train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1_supervised(model, train_loader, num_epochs=3):\n",
    "    \"\"\"\n",
    "    Stage 1: Supervised fine-tuning.\n",
    "    \n",
    "    Warm-start the dual-encoder model with supervised learning\n",
    "    before RL optimization.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STAGE 1: SUPERVISED FINE-TUNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=len(train_loader) * num_epochs\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f'{RESULTS_PATH}/logs/stage1')\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in pbar:\n",
    "            # Move to device\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                intent_input_ids=batch['intent_input_ids'],\n",
    "                intent_attention_mask=batch['intent_attention_mask'],\n",
    "                k8s_input_ids=batch['k8s_input_ids'],\n",
    "                k8s_attention_mask=batch['k8s_attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "            \n",
    "            writer.add_scalar('train/loss', loss.item(), global_step)\n",
    "            writer.add_scalar('train/lr', scheduler.get_last_lr()[0], global_step)\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, f'{ENHANCED_MODEL_PATH}/stage1_epoch{epoch+1}.pt')\n",
    "    \n",
    "    writer.close()\n",
    "    print(\"\\n✓ Stage 1 complete!\")\n",
    "    return model\n",
    "\n",
    "# Run Stage 1\n",
    "model = train_stage1_supervised(model, train_loader, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2_rl(model, train_loader, num_epochs=2):\n",
    "    \"\"\"\n",
    "    Stage 2: RL optimization.\n",
    "    \n",
    "    Fine-tune with policy gradient to maximize reward signal.\n",
    "    Target: BP% ≥90%, Quality ≥85/100\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STAGE 2: REINFORCEMENT LEARNING OPTIMIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Target: BP% ≥90%, Quality Score ≥85/100\")\n",
    "    \n",
    "    trainer = PolicyGradientTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        reward_calculator=reward_calculator,\n",
    "        learning_rate=1e-5\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f'{RESULTS_PATH}/logs/stage2')\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_metrics = defaultdict(list)\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"RL Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in pbar:\n",
    "            # Prepare inputs\n",
    "            intent_batch = {\n",
    "                'input_ids': batch['intent_input_ids'].to(device),\n",
    "                'attention_mask': batch['intent_attention_mask'].to(device)\n",
    "            }\n",
    "            k8s_batch = {\n",
    "                'input_ids': batch['k8s_input_ids'].to(device),\n",
    "                'attention_mask': batch['k8s_attention_mask'].to(device)\n",
    "            }\n",
    "            target_yamls = batch['target_yaml']\n",
    "            \n",
    "            # Train episode\n",
    "            episode_metrics = trainer.train_episode(\n",
    "                intent_batch, \n",
    "                k8s_batch, \n",
    "                target_yamls\n",
    "            )\n",
    "            \n",
    "            # Track metrics\n",
    "            for k, v in episode_metrics.items():\n",
    "                epoch_metrics[k].append(v)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Log to tensorboard\n",
    "            writer.add_scalar('rl/reward', episode_metrics['avg_reward'], global_step)\n",
    "            writer.add_scalar('rl/bp_score', episode_metrics['avg_bp_score'], global_step)\n",
    "            writer.add_scalar('rl/loss', episode_metrics['pg_loss'], global_step)\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'reward': f\"{episode_metrics['avg_reward']:.3f}\",\n",
    "                'BP%': f\"{episode_metrics['avg_bp_score']:.1f}\"\n",
    "            })\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_reward = np.mean(epoch_metrics['avg_reward'])\n",
    "        avg_bp = np.mean(epoch_metrics['avg_bp_score'])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Avg Reward: {avg_reward:.3f}\")\n",
    "        print(f\"  Avg BP%: {avg_bp:.1f}%\")\n",
    "        print(f\"  Target BP%: 90%\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'avg_reward': avg_reward,\n",
    "            'avg_bp_score': avg_bp,\n",
    "        }, f'{ENHANCED_MODEL_PATH}/stage2_epoch{epoch+1}.pt')\n",
    "    \n",
    "    writer.close()\n",
    "    print(\"\\n✓ Stage 2 complete!\")\n",
    "    return model\n",
    "\n",
    "# Run Stage 2\n",
    "model = train_stage2_rl(model, train_loader, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Comparison\n",
    "\n",
    "Compare:\n",
    "- Baseline CodeT5 (from Phase 1)\n",
    "- Enhanced Dual-Encoder + RL (Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, model_name=\"Enhanced\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of model.\n",
    "    \n",
    "    Metrics:\n",
    "    - CodeBLEU\n",
    "    - YAML Validity\n",
    "    - Best Practices %\n",
    "    - Quality Score\n",
    "    - Security Score\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING: {model_name} Model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    results = {\n",
    "        'codebleu_scores': [],\n",
    "        'bp_percentages': [],\n",
    "        'quality_scores': [],\n",
    "        'security_scores': [],\n",
    "        'validity_checks': [],\n",
    "        'generated_yamls': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move to device\n",
    "            intent_batch = {\n",
    "                'input_ids': batch['intent_input_ids'].to(device),\n",
    "                'attention_mask': batch['intent_attention_mask'].to(device)\n",
    "            }\n",
    "            k8s_batch = {\n",
    "                'input_ids': batch['k8s_input_ids'].to(device),\n",
    "                'attention_mask': batch['k8s_attention_mask'].to(device)\n",
    "            }\n",
    "            \n",
    "            # Generate\n",
    "            generated_ids = model.generate(\n",
    "                intent_input_ids=intent_batch['input_ids'],\n",
    "                intent_attention_mask=intent_batch['attention_mask'],\n",
    "                k8s_input_ids=k8s_batch['input_ids'],\n",
    "                k8s_attention_mask=k8s_batch['attention_mask'],\n",
    "                max_length=512,\n",
    "                num_beams=4\n",
    "            )\n",
    "            \n",
    "            # Decode\n",
    "            generated_yamls = [\n",
    "                tokenizer.decode(ids, skip_special_tokens=True)\n",
    "                for ids in generated_ids\n",
    "            ]\n",
    "            \n",
    "            # Evaluate each generated YAML\n",
    "            for gen_yaml, target_yaml in zip(generated_yamls, batch['target_yaml']):\n",
    "                # Compute metrics\n",
    "                reward_dict = reward_calculator.compute_reward(gen_yaml, target_yaml)\n",
    "                \n",
    "                # Check YAML validity\n",
    "                is_valid = False\n",
    "                try:\n",
    "                    yaml.safe_load(gen_yaml)\n",
    "                    is_valid = True\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Store results\n",
    "                results['codebleu_scores'].append(reward_dict['codebleu_score'])\n",
    "                results['bp_percentages'].append(reward_dict['bp_percentage'])\n",
    "                results['security_scores'].append(reward_dict['security_score'])\n",
    "                results['validity_checks'].append(is_valid)\n",
    "                results['generated_yamls'].append(gen_yaml)\n",
    "                \n",
    "                # Compute quality score\n",
    "                quality = (reward_dict['bp_percentage'] * 0.7 + \n",
    "                          reward_dict['codebleu_score'] * 100 * 0.3)\n",
    "                results['quality_scores'].append(quality)\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    summary = {\n",
    "        'model': model_name,\n",
    "        'codebleu_mean': np.mean(results['codebleu_scores']) * 100,\n",
    "        'codebleu_std': np.std(results['codebleu_scores']) * 100,\n",
    "        'bp_mean': np.mean(results['bp_percentages']),\n",
    "        'bp_std': np.std(results['bp_percentages']),\n",
    "        'quality_mean': np.mean(results['quality_scores']),\n",
    "        'quality_std': np.std(results['quality_scores']),\n",
    "        'security_mean': np.mean(results['security_scores']) * 100,\n",
    "        'validity_rate': np.mean(results['validity_checks']) * 100\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  CodeBLEU:     {summary['codebleu_mean']:.2f}% (±{summary['codebleu_std']:.2f})\")\n",
    "    print(f\"  BP%:          {summary['bp_mean']:.2f}% (±{summary['bp_std']:.2f})\")\n",
    "    print(f\"  Quality:      {summary['quality_mean']:.2f}/100 (±{summary['quality_std']:.2f})\")\n",
    "    print(f\"  Security:     {summary['security_mean']:.2f}%\")\n",
    "    print(f\"  YAML Valid:   {summary['validity_rate']:.2f}%\")\n",
    "    \n",
    "    return summary, results\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_dataset = DualEncoderDataset(\n",
    "    data_path=f'{DATA_PATH}/test_data.json',\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate enhanced model\n",
    "enhanced_summary, enhanced_results = evaluate_model(\n",
    "    model, \n",
    "    test_loader, \n",
    "    model_name=\"Enhanced (Dual-Encoder + RL)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model for comparison\n",
    "print(\"\\nLoading baseline model...\")\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(BASELINE_MODEL_PATH)\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "# Note: Baseline uses single encoder, so we need to adapt inputs\n",
    "# For fair comparison, concatenate intent + k8s pattern for baseline\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_summary, baseline_results = evaluate_model(\n",
    "    baseline_model,\n",
    "    test_loader,\n",
    "    model_name=\"Baseline (Standard CodeT5)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def compare_models(baseline_results, enhanced_results):\n",
    "    \"\"\"\n",
    "    Statistical comparison of baseline vs enhanced model.\n",
    "    \n",
    "    Uses paired t-test to determine if improvements are significant.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metrics = ['codebleu_scores', 'bp_percentages', 'quality_scores']\n",
    "    metric_names = ['CodeBLEU', 'Best Practices %', 'Quality Score']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for metric, name in zip(metrics, metric_names):\n",
    "        baseline_vals = np.array(baseline_results[metric])\n",
    "        enhanced_vals = np.array(enhanced_results[metric])\n",
    "        \n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(enhanced_vals, baseline_vals)\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        diff = enhanced_vals - baseline_vals\n",
    "        cohens_d = np.mean(diff) / np.std(diff)\n",
    "        \n",
    "        # Improvement percentage\n",
    "        improvement = ((np.mean(enhanced_vals) - np.mean(baseline_vals)) / \n",
    "                      np.mean(baseline_vals)) * 100\n",
    "        \n",
    "        results[metric] = {\n",
    "            'baseline_mean': np.mean(baseline_vals),\n",
    "            'enhanced_mean': np.mean(enhanced_vals),\n",
    "            'improvement_pct': improvement,\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'cohens_d': cohens_d,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Baseline:     {np.mean(baseline_vals):.3f}\")\n",
    "        print(f\"  Enhanced:     {np.mean(enhanced_vals):.3f}\")\n",
    "        print(f\"  Improvement:  {improvement:+.2f}%\")\n",
    "        print(f\"  t-statistic:  {t_stat:.3f}\")\n",
    "        print(f\"  p-value:      {p_value:.4f}\")\n",
    "        print(f\"  Cohen's d:    {cohens_d:.3f}\")\n",
    "        print(f\"  Significant:  {'✓ YES' if p_value < 0.05 else '✗ NO'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run statistical comparison\n",
    "stat_results = compare_models(baseline_results, enhanced_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. CodeBLEU Comparison\n",
    "ax = axes[0, 0]\n",
    "models = ['Baseline', 'Enhanced']\n",
    "codebleu_means = [\n",
    "    baseline_summary['codebleu_mean'],\n",
    "    enhanced_summary['codebleu_mean']\n",
    "]\n",
    "ax.bar(models, codebleu_means, color=['#3498db', '#2ecc71'])\n",
    "ax.axhline(y=85, color='r', linestyle='--', label='Target: 85%')\n",
    "ax.set_ylabel('CodeBLEU Score (%)')\n",
    "ax.set_title('CodeBLEU Comparison')\n",
    "ax.legend()\n",
    "\n",
    "# 2. Best Practices % Comparison  \n",
    "ax = axes[0, 1]\n",
    "bp_means = [\n",
    "    baseline_summary['bp_mean'],\n",
    "    enhanced_summary['bp_mean']\n",
    "]\n",
    "ax.bar(models, bp_means, color=['#3498db', '#2ecc71'])\n",
    "ax.axhline(y=90, color='r', linestyle='--', label='Target: 90%')\n",
    "ax.set_ylabel('Best Practices Score (%)')\n",
    "ax.set_title('Best Practices % Comparison')\n",
    "ax.legend()\n",
    "\n",
    "# 3. Quality Score Comparison\n",
    "ax = axes[1, 0]\n",
    "quality_means = [\n",
    "    baseline_summary['quality_mean'],\n",
    "    enhanced_summary['quality_mean']\n",
    "]\n",
    "ax.bar(models, quality_means, color=['#3498db', '#2ecc71'])\n",
    "ax.axhline(y=85, color='r', linestyle='--', label='Target: 85/100')\n",
    "ax.set_ylabel('Quality Score (/100)')\n",
    "ax.set_title('Overall Quality Comparison')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Multi-metric Radar Chart\n",
    "ax = axes[1, 1]\n",
    "categories = ['CodeBLEU', 'BP%', 'Quality', 'Security', 'Validity']\n",
    "baseline_values = [\n",
    "    baseline_summary['codebleu_mean'],\n",
    "    baseline_summary['bp_mean'],\n",
    "    baseline_summary['quality_mean'],\n",
    "    baseline_summary['security_mean'],\n",
    "    baseline_summary['validity_rate']\n",
    "]\n",
    "enhanced_values = [\n",
    "    enhanced_summary['codebleu_mean'],\n",
    "    enhanced_summary['bp_mean'],\n",
    "    enhanced_summary['quality_mean'],\n",
    "    enhanced_summary['security_mean'],\n",
    "    enhanced_summary['validity_rate']\n",
    "]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, baseline_values, width, label='Baseline', color='#3498db')\n",
    "ax.bar(x + width/2, enhanced_values, width, label='Enhanced', color='#2ecc71')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Multi-Metric Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/comparison_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final results\n",
    "final_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_info': {\n",
    "        'architecture': 'Dual-Encoder CodeT5 with RL',\n",
    "        'base_model': 'Salesforce/codet5-base',\n",
    "        'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'training_stages': ['Supervised Fine-tuning', 'RL Optimization']\n",
    "    },\n",
    "    'baseline': baseline_summary,\n",
    "    'enhanced': enhanced_summary,\n",
    "    'statistical_tests': stat_results,\n",
    "    'targets_achieved': {\n",
    "        'codebleu_target_85': enhanced_summary['codebleu_mean'] >= 85,\n",
    "        'bp_target_90': enhanced_summary['bp_mean'] >= 90,\n",
    "        'quality_target_85': enhanced_summary['quality_mean'] >= 85\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(f'{RESULTS_PATH}/phase2_final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': model.config,\n",
    "    'results': final_results\n",
    "}, f'{ENHANCED_MODEL_PATH}/final_enhanced_model.pt')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults saved to: {RESULTS_PATH}\")\n",
    "print(f\"Model saved to: {ENHANCED_MODEL_PATH}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n📊 FINAL SUMMARY:\")\n",
    "print(f\"\\nBaseline Performance:\")\n",
    "print(f\"  CodeBLEU: {baseline_summary['codebleu_mean']:.2f}%\")\n",
    "print(f\"  BP%:      {baseline_summary['bp_mean']:.2f}%\")\n",
    "print(f\"  Quality:  {baseline_summary['quality_mean']:.2f}/100\")\n",
    "\n",
    "print(f\"\\nEnhanced Performance:\")\n",
    "print(f\"  CodeBLEU: {enhanced_summary['codebleu_mean']:.2f}% {'✓' if enhanced_summary['codebleu_mean'] >= 85 else '✗'}\")\n",
    "print(f\"  BP%:      {enhanced_summary['bp_mean']:.2f}% {'✓' if enhanced_summary['bp_mean'] >= 90 else '✗'}\")\n",
    "print(f\"  Quality:  {enhanced_summary['quality_mean']:.2f}/100 {'✓' if enhanced_summary['quality_mean'] >= 85 else '✗'}\")\n",
    "\n",
    "print(f\"\\n🎯 Targets: CodeBLEU ≥85%, BP% ≥90%, Quality ≥85/100\")\n",
    "targets_met = sum(final_results['targets_achieved'].values())\n",
    "print(f\"   Status: {targets_met}/3 targets achieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "### Phase 3 (Weeks 7-8): Evaluation & Thesis Writing\n",
    "\n",
    "1. **Comprehensive Evaluation** (4 days)\n",
    "   - Expanded test set evaluation\n",
    "   - Ablation studies (contribution of each component)\n",
    "   - Error analysis\n",
    "\n",
    "2. **User Study** (3 days)\n",
    "   - Gather feedback from DevOps engineers\n",
    "   - Evaluate practical utility\n",
    "   - Assess feedback quality\n",
    "\n",
    "3. **Thesis Documentation** (remaining time)\n",
    "   - Results & analysis\n",
    "   - Discussion\n",
    "   - Conclusions & future work\n",
    "\n",
    "### Key Contributions to Highlight:\n",
    "\n",
    "1. **Architectural Innovation**: First dual-encoder approach for IaC generation\n",
    "2. **RL Optimization**: Novel reward function based on domain-specific quality metrics\n",
    "3. **Actionable Feedback**: Transforms generation into education\n",
    "4. **Empirical Validation**: Systematic comparison with statistical significance testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
